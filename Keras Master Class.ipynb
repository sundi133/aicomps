{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras: Deep Learning library for Theano and TensorFlow\n",
    "\n",
    "> Keras is a high-level neural network library, written in Python and capable of running on top of either TensorFlow or Theano. \n",
    "\n",
    "> It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is install Keras. Run the code below in the anaconda prompt. \n",
    "\n",
    "> conda install -c conda-forge keras=2.0.2\n",
    "\n",
    "When you receive the Theano warning. Paste this into the anaconda prompt. \n",
    "\n",
    "> conda install m2w64-toolchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-8c1c6fe602a5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8c1c6fe602a5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    conda install -c conda-forge keras=2.0.2\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge keras=2.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is our installation path? \n",
    "\n",
    "> What backend are we using? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python27.zip', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/plat-darwin', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/plat-mac', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/lib-tk', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/lib-old', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/lib-dynload', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/site-packages', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg', '/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/site-packages/IPython/extensions', '/Users/jyotirmoysundi/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "#keras.backend.backend()\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the version of our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('numpy:', '1.13.1')\n",
      "('scipy:', '0.19.1')\n",
      "('matplotlib:', '2.1.0')\n",
      "('iPython:', '5.5.0')\n",
      "('scikit-learn:', '0.19.0')\n",
      "('keras: ', '2.0.2')\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print('numpy:', numpy.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "\n",
    "import IPython\n",
    "print('iPython:', IPython.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "\n",
    "import keras\n",
    "print('keras: ', keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - The Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Bring in libraries or modules we will use in building our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotirmoysundi/miniconda2/envs/ass2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Set a random seed. Simply for reproducability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7 \n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Import and separate our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = numpy.loadtxt(\"pima.csv\", delimiter=\",\") \n",
    "X = ds[:,0:8] \n",
    "Y = ds[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras updated their API yesterday (March 14, 2017) to 2.0 version. Obviously you have downloaded that version and the demo still uses the \"old\" API. They have created warnings so that the \"old\" API would still work in the version 2.0, but saying that it will change so please use 2.0 API from now on.\n",
    "\n",
    "> The way to adapt your code to API 2.0 is to change the \"init\" parameter to \"kernel_initializer\" for all of the Dense() layers as well as the \"nb_epoch\" to \"epochs\" in the fit() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Defing the Keras Components\n",
    "\n",
    "The first thing we do is to use the keyword Sequential. It's like a container for our model. On the next few line we see 'model.add.' This is how we add layers to neural network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu')) \n",
    "model.add(Dense(8, kernel_initializer ='uniform', activation='sigmoid')) \n",
    "model.add(Dense(1, kernel_initializer ='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Compile our model\n",
    "\n",
    "Now that the model is deﬁned, we can compile it. Compiling the model uses numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow.\n",
    "\n",
    "We are using the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. A loss function is a way of measuring how well your classification or regression algorithm is working. A high value for the loss function means that you are not classifying things well or that your predictions are far from the true values, and a low value means that your algorithm works well.\n",
    "\n",
    "The model also uses the efficient Adam optimization algorithm for gradient descent.\n",
    "\n",
    "A metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the metrics parameter when a model is compiled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Fit our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to execute the model on some data. We can train or ﬁt our model on our loaded data by calling the fit() function on the model.\n",
    "\n",
    "> The training process will run for a ﬁxed number of iterations through the dataset called epochs, that we must specify using the epochs argument. \n",
    "\n",
    "We can also set the number of instances that are evaluated before a weight update in the network is performed called the batch size and set using the batch size argument. \n",
    "\n",
    "> Note: This is where the CPU or GPU burn will happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 691 samples, validate on 77 samples\n",
      "Epoch 1/50\n",
      "691/691 [==============================] - 0s - loss: 1.2674 - acc: 0.0000e+00 - val_loss: 0.7004 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "691/691 [==============================] - 0s - loss: 0.4428 - acc: 0.0000e+00 - val_loss: -0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "691/691 [==============================] - 0s - loss: -0.3917 - acc: 0.0000e+00 - val_loss: -1.0396 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "691/691 [==============================] - 0s - loss: -1.2833 - acc: 0.0000e+00 - val_loss: -2.0207 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "691/691 [==============================] - 0s - loss: -2.2551 - acc: 0.0000e+00 - val_loss: -3.0973 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "691/691 [==============================] - 0s - loss: -3.3177 - acc: 0.0000e+00 - val_loss: -4.2596 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "691/691 [==============================] - 0s - loss: -4.4561 - acc: 0.0000e+00 - val_loss: -5.4786 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "691/691 [==============================] - 0s - loss: -5.6363 - acc: 0.0000e+00 - val_loss: -6.7283 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "691/691 [==============================] - 0s - loss: -6.8611 - acc: 0.0000e+00 - val_loss: -7.9942 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "691/691 [==============================] - 0s - loss: -8.0836 - acc: 0.0000e+00 - val_loss: -9.2426 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "691/691 [==============================] - 0s - loss: -9.3027 - acc: 0.0000e+00 - val_loss: -10.4681 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "691/691 [==============================] - 0s - loss: -10.4969 - acc: 0.0000e+00 - val_loss: -11.7177 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "691/691 [==============================] - 0s - loss: -11.7173 - acc: 0.0000e+00 - val_loss: -12.9641 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "691/691 [==============================] - 0s - loss: -12.9302 - acc: 0.0000e+00 - val_loss: -14.1760 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "691/691 [==============================] - 0s - loss: -14.0928 - acc: 0.0000e+00 - val_loss: -15.3627 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "691/691 [==============================] - 0s - loss: -15.2400 - acc: 0.0000e+00 - val_loss: -16.5258 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "691/691 [==============================] - 0s - loss: -16.3683 - acc: 0.0000e+00 - val_loss: -17.6732 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "691/691 [==============================] - 0s - loss: -17.4759 - acc: 0.0000e+00 - val_loss: -18.8039 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "691/691 [==============================] - 0s - loss: -18.5744 - acc: 0.0000e+00 - val_loss: -19.9216 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "691/691 [==============================] - 0s - loss: -19.6485 - acc: 0.0000e+00 - val_loss: -21.0322 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "691/691 [==============================] - 0s - loss: -20.7298 - acc: 0.0000e+00 - val_loss: -22.1275 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "691/691 [==============================] - 0s - loss: -21.7930 - acc: 0.0000e+00 - val_loss: -23.2140 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "691/691 [==============================] - 0s - loss: -22.8434 - acc: 0.0000e+00 - val_loss: -24.2933 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "691/691 [==============================] - 0s - loss: -23.8915 - acc: 0.0000e+00 - val_loss: -25.3642 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "691/691 [==============================] - 0s - loss: -24.9262 - acc: 0.0000e+00 - val_loss: -26.4296 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "691/691 [==============================] - 0s - loss: -25.9532 - acc: 0.0000e+00 - val_loss: -27.4879 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "691/691 [==============================] - 0s - loss: -26.9812 - acc: 0.0000e+00 - val_loss: -28.5381 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "691/691 [==============================] - 0s - loss: -28.0007 - acc: 0.0000e+00 - val_loss: -29.5802 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "691/691 [==============================] - 0s - loss: -29.0098 - acc: 0.0000e+00 - val_loss: -30.6190 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "691/691 [==============================] - 0s - loss: -30.0147 - acc: 0.0000e+00 - val_loss: -31.6518 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "691/691 [==============================] - 0s - loss: -31.0220 - acc: 0.0000e+00 - val_loss: -32.6773 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "691/691 [==============================] - 0s - loss: -32.0070 - acc: 0.0000e+00 - val_loss: -33.7026 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "691/691 [==============================] - 0s - loss: -33.0048 - acc: 0.0000e+00 - val_loss: -34.7203 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "691/691 [==============================] - 0s - loss: -33.9941 - acc: 0.0000e+00 - val_loss: -35.7362 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "691/691 [==============================] - 0s - loss: -34.9788 - acc: 0.0000e+00 - val_loss: -36.7496 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "691/691 [==============================] - 0s - loss: -35.9649 - acc: 0.0000e+00 - val_loss: -37.7583 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "691/691 [==============================] - 0s - loss: -36.9434 - acc: 0.0000e+00 - val_loss: -38.7674 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "691/691 [==============================] - 0s - loss: -37.9209 - acc: 0.0000e+00 - val_loss: -39.7745 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "691/691 [==============================] - 0s - loss: -38.8973 - acc: 0.0000e+00 - val_loss: -40.7786 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "691/691 [==============================] - 0s - loss: -39.8713 - acc: 0.0000e+00 - val_loss: -41.7790 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "691/691 [==============================] - 0s - loss: -40.8441 - acc: 0.0000e+00 - val_loss: -42.7782 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "691/691 [==============================] - 0s - loss: -41.8119 - acc: 0.0000e+00 - val_loss: -43.7768 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "691/691 [==============================] - 0s - loss: -42.7762 - acc: 0.0000e+00 - val_loss: -44.7737 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "691/691 [==============================] - 0s - loss: -43.7460 - acc: 0.0000e+00 - val_loss: -45.7654 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "691/691 [==============================] - 0s - loss: -44.7084 - acc: 0.0000e+00 - val_loss: -46.7535 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "691/691 [==============================] - 0s - loss: -45.6694 - acc: 0.0000e+00 - val_loss: -47.7397 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "691/691 [==============================] - 0s - loss: -46.6199 - acc: 0.0000e+00 - val_loss: -48.7257 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "691/691 [==============================] - 0s - loss: -47.5807 - acc: 0.0000e+00 - val_loss: -49.7065 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "691/691 [==============================] - 0s - loss: -48.5326 - acc: 0.0000e+00 - val_loss: -50.6867 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "691/691 [==============================] - 0s - loss: -49.4845 - acc: 0.0000e+00 - val_loss: -51.6679 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c6bab90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.1, epochs=50, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last step we have trained our keras model on the entire dataset and we can evaluate the performance of the network on the same dataset. \n",
    "\n",
    ">This will only give us an idea of how well we have modeled the dataset but no idea of how well the algorithm might perform on new data. \n",
    "\n",
    "We have done this for simplicity, but ideally, you could separate your data into train and test datasets for the training and evaluation of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0sacc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X,Y) \n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras way:\n",
    "\n",
    "> The core data structure of Keras is a model, a way to organize layers. The main type of model is the Sequential model, a linear stack of layers.\n",
    "\n",
    "What we did here is stacking a Fully Connected (Dense) layer of trainable weights from the input to the output and an Activation layer on top of the weights layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Evaluating Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides two convenient ways of evaluating your deep learning algorithms this way.\n",
    "\n",
    "> Use an automatic veriﬁcation dataset.\n",
    "\n",
    "> Use a manual veriﬁcation dataset.\n",
    "\n",
    "Often times these are tested with different values. \n",
    "\n",
    "The large amount of data and the complexity of the models require very long training times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How is the validation split computed? **\n",
    "\n",
    "If you set the validation_split argument in model.fit to .10, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This is where we define **automatic verfication** against our data set. \n",
    "\n",
    "In the example below we set validation_split to .25. That means we are holding back **25%** of our data for validation. That also means we are training on 75% of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/20\n",
      "576/576 [==============================] - 0s - loss: -53.8752 - acc: 0.0000e+00 - val_loss: -58.9474 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "576/576 [==============================] - 0s - loss: -62.9589 - acc: 0.0000e+00 - val_loss: -68.1388 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "576/576 [==============================] - 0s - loss: -71.8827 - acc: 0.0000e+00 - val_loss: -76.8529 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "576/576 [==============================] - 0s - loss: -80.3911 - acc: 0.0000e+00 - val_loss: -85.3005 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "576/576 [==============================] - 0s - loss: -88.6482 - acc: 0.0000e+00 - val_loss: -93.6314 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "576/576 [==============================] - 0s - loss: -96.8120 - acc: 0.0000e+00 - val_loss: -101.7207 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "576/576 [==============================] - 0s - loss: -104.8297 - acc: 0.0000e+00 - val_loss: -109.7555 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "576/576 [==============================] - 0s - loss: -112.7935 - acc: 0.0000e+00 - val_loss: -117.6677 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "576/576 [==============================] - 0s - loss: -120.6581 - acc: 0.0000e+00 - val_loss: -125.6047 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "576/576 [==============================] - 0s - loss: -128.4805 - acc: 0.0000e+00 - val_loss: -133.5466 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "576/576 [==============================] - 0s - loss: -136.3243 - acc: 0.0000e+00 - val_loss: -141.3203 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "576/576 [==============================] - 0s - loss: -144.0747 - acc: 0.0000e+00 - val_loss: -149.1050 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "576/576 [==============================] - 0s - loss: -151.8160 - acc: 0.0000e+00 - val_loss: -156.8758 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "576/576 [==============================] - 0s - loss: -159.5225 - acc: 0.0000e+00 - val_loss: -164.6329 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "576/576 [==============================] - 0s - loss: -167.2140 - acc: 0.0000e+00 - val_loss: -172.4016 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "576/576 [==============================] - 0s - loss: -174.8969 - acc: 0.0000e+00 - val_loss: -180.1293 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "576/576 [==============================] - 0s - loss: -182.5828 - acc: 0.0000e+00 - val_loss: -187.7624 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "576/576 [==============================] - 0s - loss: -190.1898 - acc: 0.0000e+00 - val_loss: -195.5054 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "576/576 [==============================] - 0s - loss: -197.8303 - acc: 0.0000e+00 - val_loss: -203.2031 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "576/576 [==============================] - 0s - loss: -205.4540 - acc: 0.0000e+00 - val_loss: -210.8667 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c9ac410>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.25, epochs=20, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This is where we define **Manual verfication** against our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 0s - loss: -2.6257 - acc: 0.0000e+00 - val_loss: -18.0172 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s - loss: -112.3436 - acc: 0.0000e+00 - val_loss: -338.8178 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s - loss: -456.0238 - acc: 0.0000e+00 - val_loss: -526.9654 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s - loss: -503.7716 - acc: 0.0000e+00 - val_loss: -528.9922 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s - loss: -505.3630 - acc: 0.0000e+00 - val_loss: -530.4397 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s - loss: -506.1454 - acc: 0.0000e+00 - val_loss: -531.0772 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s - loss: -506.6563 - acc: 0.0000e+00 - val_loss: -531.6483 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s - loss: -506.8748 - acc: 0.0000e+00 - val_loss: -531.7677 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s - loss: -507.0390 - acc: 0.0000e+00 - val_loss: -531.8184 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s - loss: -507.1298 - acc: 0.0000e+00 - val_loss: -531.8490 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s - loss: -507.2087 - acc: 0.0000e+00 - val_loss: -531.8759 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s - loss: -507.2498 - acc: 0.0000e+00 - val_loss: -531.9079 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s - loss: -507.3222 - acc: 0.0000e+00 - val_loss: -531.9365 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s - loss: -507.3684 - acc: 0.0000e+00 - val_loss: -531.9683 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s - loss: -507.4124 - acc: 0.0000e+00 - val_loss: -532.0033 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      " 10/576 [..............................] - ETA: 0s - loss: -420.0084 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy \n",
    "\n",
    "seed = 7 \n",
    "numpy.random.seed(seed)\n",
    "\n",
    "ds = numpy.loadtxt(\"pima.csv\", delimiter=\",\") \n",
    "X = ds[:,0:8] \n",
    "Y = ds[:,8]   \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=seed) \n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu')) \n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu')) \n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X,Y) \n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** init has been changed in 2.0 to kernel_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a for loop that defines the model, fits it, scores it for each fold\n",
    "\n",
    "Also keep in mind that kfold cross validation isn't Keras. We are using SciKit-Learn here. \n",
    "\n",
    ">Cross-validation is often not used for evaluating deep learning models because of the greater computational expense. For example k-fold cross-validation is often used with 10 folds. \n",
    "\n",
    "That translates into 10 models that must be constructed and evaluated, greatly adding to the evaluation time of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy \n",
    "\n",
    "seed = 7 \n",
    "numpy.random.seed(seed)\n",
    "\n",
    "ds = numpy.loadtxt(\"pima.csv\", delimiter=\",\") \n",
    "X = ds[:,0:8] \n",
    "Y = ds[:,8]   \n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, Y): \n",
    "    \n",
    " model = Sequential() \n",
    " model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu')) \n",
    " model.add(Dense(8, kernel_initializer='uniform', activation='relu')) \n",
    " model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid')) \n",
    "\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    " model.fit(X[train], Y[train], epochs=10, batch_size=10, verbose=0) \n",
    " scores = model.evaluate(X[test], Y[test], verbose=0) \n",
    " print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Use Grid Search in scikit-learn\n",
    "\n",
    "> The Keras library provides a convenient wrapper for deep learning models to be used as classiﬁcation or regression estimators in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import our Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a wrapper for our Keras model\n",
    "   > def  < Start of function\n",
    "   \n",
    "   > return < end of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential() \n",
    "    model.add(Dense(12, input_dim=8, init=init, activation='relu')) \n",
    "    model.add(Dense(8, init=init, activation='relu')) \n",
    "    model.add(Dense(1, init=init, activation='sigmoid')) \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Set out seed and import out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 55\n",
    "numpy.random.seed(seed) \n",
    "dataset = numpy.loadtxt(\"pima.csv\", delimiter=\",\") \n",
    "X = dataset[:,0:8] \n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: This is where we create the model. Take note of the build_fn function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['rmsprop', 'adam'] \n",
    "init = ['glorot_uniform', 'normal', 'uniform'] \n",
    "epochs = [50, 100, 150] \n",
    "batches = [5, 10, 20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init) \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid) \n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 - Using Keras with Other Core Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keras is a library specifc to deep learning. We need to use other libraries like SciKit-Learn to expand how we build and deploy our mdoels. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture let's buiod a binary classification model. \n",
    "\n",
    "This simply means our output will be a 1 or a 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.13% (5.20%)\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from pandas import read_csv \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 7 \n",
    "numpy.random.seed(seed) \n",
    "dataframe = read_csv(\"sonar.csv\") \n",
    "dataset = dataframe.values \n",
    "\n",
    "X = dataset[:,0:60].astype(float) \n",
    "Y = dataset[:,60] \n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(Y) \n",
    "encoded_Y = encoder.transform(Y) \n",
    "\n",
    "def create_baseline(): \n",
    "        model = Sequential() \n",
    "        model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu')) \n",
    "        model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid')) \n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "        return model \n",
    "    \n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0) \n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) \n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold) \n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, input_dim=4, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67% (4.47%)\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from pandas import read_csv \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.utils import np_utils \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "seed = 7 \n",
    "numpy.random.seed(seed) \n",
    "\n",
    "dataframe = read_csv(\"iris.csv\", header=None) \n",
    "dataset = dataframe.values \n",
    "\n",
    "X = dataset[:,0:4].astype(float) \n",
    "Y = dataset[:,4] \n",
    "\n",
    "encoder = LabelEncoder() \n",
    "encoder.fit(Y) \n",
    "encoded_Y = encoder.transform(Y) \n",
    "dummy_y = np_utils.to_categorical(encoded_Y) \n",
    "\n",
    "def baseline_model(): \n",
    "        model = Sequential() \n",
    "        model.add(Dense(4, input_dim=4, kernel_initializer='normal', activation='relu')) \n",
    "        model.add(Dense(3, kernel_initializer='normal', activation='sigmoid')) \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "        return model \n",
    "    \n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0) \n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed) \n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold) \n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Checkpoints\n",
    "\n",
    "> Application checkpointing is a fault tolerance technique for long running processes. It is an approach where a snapshot of the state of the system is taken in case of system failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_acc improved from -inf to 0.63542, saving model to checkpoints-00-0.64.hdf5\n",
      "Epoch 00001: val_acc did not improve\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 00003: val_acc improved from 0.63542 to 0.64583, saving model to checkpoints-03-0.65.hdf5\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 00005: val_acc improved from 0.64583 to 0.66146, saving model to checkpoints-05-0.66.hdf5\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 00008: val_acc improved from 0.66146 to 0.66146, saving model to checkpoints-08-0.66.hdf5\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 00010: val_acc improved from 0.66146 to 0.66667, saving model to checkpoints-10-0.67.hdf5\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00012: val_acc improved from 0.66667 to 0.68229, saving model to checkpoints-12-0.68.hdf5\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 00014: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1346110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.callbacks import ModelCheckpoint \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy \n",
    "\n",
    "seed = 7 \n",
    "numpy.random.seed(seed) \n",
    "\n",
    "dataset = numpy.loadtxt(\"pima.csv\", delimiter=\",\") \n",
    "X = dataset[:,0:8] \n",
    "Y = dataset[:,8] \n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu')) \n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu')) \n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "filepath=\"checkpoints-{epoch:02d}-{val_acc:.2f}.hdf5\" \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') \n",
    "callbacks_list = [checkpoint] \n",
    "\n",
    "model.fit(X, Y, validation_split=0.25, epochs=15, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
